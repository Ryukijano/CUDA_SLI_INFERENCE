{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c8b2fefae3cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mn_gpus\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Requires at least 2 GPUs to run, but got {n_gpus}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mworld_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_gpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mrun_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo_basic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m     \u001b[0mrun_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mrun_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo_model_parallel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c8b2fefae3cf>\u001b[0m in \u001b[0;36mrun_demo\u001b[1;34m(demo_fn, world_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m              \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m              \u001b[0mnprocs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m              join=True)\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    238\u001b[0m                ' torch.multiprocessing.start_processes(...)' % start_method)\n\u001b[0;32m    239\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'spawn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    152\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                     \u001b[0mexit_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m                 )\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR']='localhost'\n",
    "    os.environt['MASTER_PORT']= '12355'\n",
    "    \n",
    "    #intialising the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "\n",
    "#creating toymodel\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10,10)\n",
    "        self.relu = nn.ReLu()\n",
    "        self.net2 = nn.Linear(10,5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    #creatimg a model and moving it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SSD(ddp_model.parameters(), lr=0.001)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20,10))\n",
    "    labels = torch.randn(20,5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "\n",
    "\n",
    "\n",
    "#save and load checkpoints\n",
    "\n",
    "def demo_checkpoint(rank, world_size):\n",
    "    print(f\"Running DDP checkpoint example on rank {rankj}.\")\n",
    "    setup(rank, world_size)\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        #all processes should see same parameters as they all start from same\n",
    "        #random parameters and gradients are synchronized in backward passes\n",
    "        #Therefore, saving in in one process is sufficient\n",
    "        \n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "        \n",
    "        #Use barrier() to make sure that process 1 loads the model after process\n",
    "        #0 saves it\n",
    "        dist.barrier()\n",
    "        #configure map_location properly\n",
    "        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "        ddp_model.load_state_dict(\n",
    "            torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
    "        \n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs= ddp_model(torch.randn(20,10))\n",
    "        labels = torch.randn(20,5).to(rank)\n",
    "        \n",
    "        loss_fn(outputs, labels).backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #not necessary to use a dist.barrier)() to guard the file deletion \n",
    "        #as the allReduce ops in the backward pass of DDP already served as a synchronization\n",
    "        \n",
    "        if rank == 0:\n",
    "            os.remove(CHECKPOINT_PATH)\n",
    "        \n",
    "        cleanup()\n",
    "\n",
    "\n",
    "#Combing DDP with Model Parallelism\n",
    "\n",
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(TopMpModel, self). __init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10,10).to(dev0)\n",
    "        self.relu = torch.nn.Linear(10,5).to(dev1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)\n",
    "    \n",
    "    \n",
    "def demo_model_parallel(rank, world_size):\n",
    "    print(f\"Running DDP with model parallel example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # setup mp_model and devices for this process\n",
    "    dev0 = (rank * 2) % world_size\n",
    "    dev1 = (rank * 2 + 1) % world_size\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # outputs will be on dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n",
    "    world_size = n_gpus\n",
    "    run_demo(demo_basic, world_size)\n",
    "    run_demo(demo_checkpoint, world_size)\n",
    "    run_demo(demo_model_parallel, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
